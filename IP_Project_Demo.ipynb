{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Rw5_rHRU1Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d02eb9d9-4ae5-46c9-bd49-628174f5f6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.0.21)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation_models_pytorch\n",
            "Successfully installed segmentation_models_pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation_models_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SAM\n",
        "!pip install segment-anything\n",
        "\n",
        "# Download SAM checkpoint (choose one):\n",
        "# ViT-B (smallest, ~375MB)\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "# ViT-L (~1.2GB)\n",
        "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "\n",
        "# ViT-H (largest, ~2.4GB)\n",
        "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UPUg7DCsuwyl",
        "outputId": "7eb572b4-c1c3-41ac-9d2a-7640b985c1f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segment-anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n",
            "--2025-11-09 03:49:16--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.85.129.113, 52.85.129.86, 52.85.129.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.85.129.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‚Äòsam_vit_b_01ec64.pth‚Äô\n",
            "\n",
            "sam_vit_b_01ec64.pt 100%[===================>] 357.67M   257MB/s    in 1.4s    \n",
            "\n",
            "2025-11-09 03:49:17 (257 MB/s) - ‚Äòsam_vit_b_01ec64.pth‚Äô saved [375042383/375042383]\n",
            "\n",
            "--2025-11-09 03:49:18--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.85.129.113, 52.85.129.86, 52.85.129.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.85.129.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1249524607 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‚Äòsam_vit_l_0b3195.pth‚Äô\n",
            "\n",
            "sam_vit_l_0b3195.pt 100%[===================>]   1.16G   169MB/s    in 6.7s    \n",
            "\n",
            "2025-11-09 03:49:24 (178 MB/s) - ‚Äòsam_vit_l_0b3195.pth‚Äô saved [1249524607/1249524607]\n",
            "\n",
            "--2025-11-09 03:49:24--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.85.129.113, 52.85.129.86, 52.85.129.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.85.129.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‚Äòsam_vit_h_4b8939.pth‚Äô\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G  94.7MB/s    in 23s     \n",
            "\n",
            "2025-11-09 03:49:47 (107 MB/s) - ‚Äòsam_vit_h_4b8939.pth‚Äô saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GRADIO WEB APPLICATION FOR BRAIN TUMOR CLASSIFICATION AND SEGMENTATION\n",
        "Enhanced with SAM refinement for improved segmentation accuracy\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import SAM dependencies\n",
        "try:\n",
        "    from segment_anything import sam_model_registry, SamPredictor\n",
        "    SAM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: SAM not installed. Install with: pip install segment-anything\")\n",
        "    SAM_AVAILABLE = False\n",
        "\n",
        "# Add safe globals for PyTorch 2.6+\n",
        "import torch.serialization\n",
        "import numpy.core.multiarray\n",
        "torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])\n",
        "\n",
        "class BrainTumorSegmenter:\n",
        "    \"\"\"Enhanced segmentation model handler with SAM refinement\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, sam_checkpoint_path=None):\n",
        "        \"\"\"Initialize the segmenter with optional SAM refinement\"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.sam_predictor = None\n",
        "\n",
        "        # Initialize U-Net\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Segmentation model not found at: {model_path}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading segmentation model from: {model_path}\")\n",
        "\n",
        "            # Load checkpoint with weights_only=False for compatibility\n",
        "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "\n",
        "            # Extract configuration\n",
        "            self.img_size = 256  # Default\n",
        "            encoder_name = 'efficientnet-b1'  # Default\n",
        "\n",
        "            if isinstance(checkpoint, dict):\n",
        "                if 'config' in checkpoint:\n",
        "                    config = checkpoint['config']\n",
        "                    self.img_size = config.get('img_size', 256)\n",
        "                    encoder_name = config.get('encoder', 'efficientnet-b1')\n",
        "\n",
        "                if 'preprocessing' in checkpoint:\n",
        "                    prep = checkpoint['preprocessing']\n",
        "                    self.img_size = prep.get('input_size', self.img_size)\n",
        "                    self.mean = prep.get('mean', [0.485, 0.456, 0.406])\n",
        "                    self.std = prep.get('std', [0.229, 0.224, 0.225])\n",
        "                else:\n",
        "                    self.mean = [0.485, 0.456, 0.406]\n",
        "                    self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "                if 'model_architecture' in checkpoint:\n",
        "                    arch = checkpoint['model_architecture']\n",
        "                    encoder_name = arch.get('encoder_name', encoder_name)\n",
        "            else:\n",
        "                self.mean = [0.485, 0.456, 0.406]\n",
        "                self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "            # Create model\n",
        "            print(f\"  Creating U-Net with {encoder_name} encoder...\")\n",
        "            self.model = smp.Unet(\n",
        "                encoder_name=encoder_name,\n",
        "                encoder_weights=None,\n",
        "                in_channels=3,\n",
        "                classes=1,\n",
        "                activation=None,\n",
        "            )\n",
        "\n",
        "            # Load weights\n",
        "            if isinstance(checkpoint, dict):\n",
        "                if 'model_state_dict' in checkpoint:\n",
        "                    state_dict = checkpoint['model_state_dict']\n",
        "                elif 'state_dict' in checkpoint:\n",
        "                    state_dict = checkpoint['state_dict']\n",
        "                else:\n",
        "                    state_dict = checkpoint\n",
        "            else:\n",
        "                state_dict = checkpoint\n",
        "\n",
        "            # Remove 'module.' prefix if present\n",
        "            new_state_dict = {}\n",
        "            for k, v in state_dict.items():\n",
        "                if k.startswith('module.'):\n",
        "                    name = k[7:]\n",
        "                else:\n",
        "                    name = k\n",
        "                new_state_dict[name] = v\n",
        "\n",
        "            self.model.load_state_dict(new_state_dict, strict=False)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            print(f\"‚úì Segmentation model loaded successfully!\")\n",
        "\n",
        "            # Initialize SAM if available and checkpoint provided\n",
        "            if SAM_AVAILABLE and sam_checkpoint_path:\n",
        "                self._initialize_sam(sam_checkpoint_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading segmentation model: {str(e)}\")\n",
        "            self.model = None\n",
        "\n",
        "    def _initialize_sam(self, checkpoint_path):\n",
        "        \"\"\"Initialize SAM model for refinement\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(checkpoint_path):\n",
        "                print(f\"SAM checkpoint not found at: {checkpoint_path}\")\n",
        "                return\n",
        "\n",
        "            print(f\"Loading SAM model from: {checkpoint_path}\")\n",
        "\n",
        "            # Determine SAM model type based on file name\n",
        "            if 'vit_h' in checkpoint_path:\n",
        "                model_type = 'vit_h'\n",
        "            elif 'vit_l' in checkpoint_path:\n",
        "                model_type = 'vit_l'\n",
        "            elif 'vit_b' in checkpoint_path:\n",
        "                model_type = 'vit_b'\n",
        "            else:\n",
        "                model_type = 'vit_b'  # Default\n",
        "\n",
        "            sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "            sam.to(self.device)\n",
        "            self.sam_predictor = SamPredictor(sam)\n",
        "\n",
        "            print(f\"‚úì SAM model ({model_type}) loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading SAM model: {str(e)}\")\n",
        "            self.sam_predictor = None\n",
        "\n",
        "    def get_bounding_box_from_mask(self, mask):\n",
        "        \"\"\"Extract bounding box from binary mask\"\"\"\n",
        "        # Find non-zero points\n",
        "        points = np.where(mask > 0)\n",
        "\n",
        "        if len(points[0]) == 0:  # No mask found\n",
        "            return None\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        y_min = points[0].min()\n",
        "        y_max = points[0].max()\n",
        "        x_min = points[1].min()\n",
        "        x_max = points[1].max()\n",
        "\n",
        "        # Add small padding (5% of image size)\n",
        "        h, w = mask.shape\n",
        "        padding_y = int(h * 0.05)\n",
        "        padding_x = int(w * 0.05)\n",
        "\n",
        "        y_min = max(0, y_min - padding_y)\n",
        "        y_max = min(h - 1, y_max + padding_y)\n",
        "        x_min = max(0, x_min - padding_x)\n",
        "        x_max = min(w - 1, x_max + padding_x)\n",
        "\n",
        "        return np.array([x_min, y_min, x_max, y_max])\n",
        "\n",
        "    def refine_with_sam(self, image_np, initial_mask):\n",
        "        \"\"\"Refine segmentation using SAM with bounding box prompt\"\"\"\n",
        "        if self.sam_predictor is None:\n",
        "            return initial_mask\n",
        "\n",
        "        try:\n",
        "            # Get bounding box from initial mask\n",
        "            bbox = self.get_bounding_box_from_mask(initial_mask)\n",
        "\n",
        "            if bbox is None:\n",
        "                return initial_mask\n",
        "\n",
        "            # Set image for SAM\n",
        "            self.sam_predictor.set_image(image_np)\n",
        "\n",
        "            # Get mask using box prompt\n",
        "            masks, scores, _ = self.sam_predictor.predict(\n",
        "                box=bbox,\n",
        "                multimask_output=True  # Get multiple mask options\n",
        "            )\n",
        "\n",
        "            # Select best mask (highest score)\n",
        "            best_idx = np.argmax(scores)\n",
        "            refined_mask = masks[best_idx].astype(np.uint8)\n",
        "\n",
        "            # Optionally combine with initial mask (intersection or union)\n",
        "            # Here we'll use intersection to be more conservative\n",
        "            combined_mask = np.logical_and(refined_mask, initial_mask).astype(np.uint8)\n",
        "\n",
        "            # If combined mask is too small, use the refined mask alone\n",
        "            if combined_mask.sum() < initial_mask.sum() * 0.3:\n",
        "                return refined_mask\n",
        "\n",
        "            return combined_mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"SAM refinement error: {str(e)}\")\n",
        "            return initial_mask\n",
        "\n",
        "    def segment(self, image, use_sam_refinement=True):\n",
        "        \"\"\"Segment tumor in image with optional SAM refinement\"\"\"\n",
        "        if self.model is None:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            if isinstance(image, Image.Image):\n",
        "                image_np = np.array(image.convert('RGB'))\n",
        "            else:\n",
        "                image_np = image\n",
        "\n",
        "            orig_h, orig_w = image_np.shape[:2]\n",
        "\n",
        "            # U-Net segmentation\n",
        "            transform = A.Compose([\n",
        "                A.Resize(self.img_size, self.img_size),\n",
        "                A.Normalize(mean=self.mean, std=self.std),\n",
        "                ToTensorV2(),\n",
        "            ])\n",
        "\n",
        "            transformed = transform(image=image_np)\n",
        "            image_tensor = transformed['image'].unsqueeze(0).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = self.model(image_tensor)\n",
        "                prob_map = torch.sigmoid(output).cpu().squeeze().numpy()\n",
        "\n",
        "            # Initial U-Net mask\n",
        "            binary_mask = (prob_map > 0.5).astype(np.uint8)\n",
        "\n",
        "            # Resize to original dimensions\n",
        "            binary_mask = cv2.resize(binary_mask, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
        "            prob_map = cv2.resize(prob_map, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Store U-Net mask for comparison\n",
        "            unet_mask = binary_mask.copy()\n",
        "\n",
        "            # Apply SAM refinement if available and requested\n",
        "            refined_mask = binary_mask\n",
        "            if use_sam_refinement and self.sam_predictor is not None and binary_mask.sum() > 0:\n",
        "                print(\"Applying SAM refinement...\")\n",
        "                refined_mask = self.refine_with_sam(image_np, binary_mask)\n",
        "                print(f\"Mask refined - Original pixels: {binary_mask.sum()}, Refined pixels: {refined_mask.sum()}\")\n",
        "\n",
        "            return refined_mask, prob_map, unet_mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Segmentation error: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "class BrainTumorClassifier:\n",
        "    \"\"\"Classification model handler\"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"Initialize the classifier\"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Loading classification model from: {model_path}\")\n",
        "\n",
        "        self.checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "\n",
        "        self.class_names = self.checkpoint['class_names']\n",
        "        self.num_classes = self.checkpoint['num_classes']\n",
        "        self.img_size = self.checkpoint['config']['img_size']\n",
        "        self.model_name = self.checkpoint['model_name']\n",
        "        self.test_accuracy = self.checkpoint['metrics']['test_accuracy']\n",
        "\n",
        "        self.normalize_mean = self.checkpoint['normalize_mean']\n",
        "        self.normalize_std = self.checkpoint['normalize_std']\n",
        "\n",
        "        self.model = self._create_model()\n",
        "        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        self.transform = self._create_transform()\n",
        "\n",
        "        print(f\"‚úì Classification model loaded successfully!\")\n",
        "        print(f\"  Classes: {self.class_names}\")\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"Recreate model architecture\"\"\"\n",
        "        model_classes = {\n",
        "            'resnet18': models.resnet18,\n",
        "            'resnet34': models.resnet34,\n",
        "            'resnet50': models.resnet50,\n",
        "            'resnet101': models.resnet101\n",
        "        }\n",
        "\n",
        "        model = model_classes[self.model_name](weights=None)\n",
        "        num_features = model.fc.in_features\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, self.num_classes)\n",
        "        )\n",
        "\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def _create_transform(self):\n",
        "        \"\"\"Create preprocessing transform\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=self.normalize_mean, std=self.normalize_std)\n",
        "        ])\n",
        "\n",
        "    def predict(self, image):\n",
        "        \"\"\"Make prediction on image\"\"\"\n",
        "        if image is None:\n",
        "            return None, None, None\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image).convert('RGB')\n",
        "        elif not isinstance(image, Image.Image):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(image_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
        "\n",
        "        predicted_class = self.class_names[predicted_idx.item()]\n",
        "        confidence_percent = confidence.item() * 100\n",
        "        all_probs = {\n",
        "            self.class_names[i]: prob.item() * 100\n",
        "            for i, prob in enumerate(probabilities[0])\n",
        "        }\n",
        "\n",
        "        return predicted_class, confidence_percent, all_probs\n",
        "\n",
        "# Global variables for models\n",
        "classifier = None\n",
        "segmenter = None\n",
        "\n",
        "def load_sample_images():\n",
        "    \"\"\"Load sample images from the specified directory\"\"\"\n",
        "    sample_dir = \"/content/drive/MyDrive/IP Project/sample_images\"\n",
        "    sample_images = []\n",
        "\n",
        "    if os.path.exists(sample_dir):\n",
        "        print(f\"Loading sample images from: {sample_dir}\")\n",
        "\n",
        "        # Get all image files\n",
        "        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff', '*.bmp']\n",
        "        image_files = []\n",
        "\n",
        "        for ext in image_extensions:\n",
        "            image_files.extend(glob.glob(os.path.join(sample_dir, ext)))\n",
        "            image_files.extend(glob.glob(os.path.join(sample_dir, ext.upper())))\n",
        "\n",
        "        # Sort for consistent ordering\n",
        "        image_files = sorted(image_files)\n",
        "\n",
        "        # Limit to reasonable number of samples\n",
        "        max_samples = 10\n",
        "        for img_path in image_files[:max_samples]:\n",
        "            try:\n",
        "                # Verify the image can be opened\n",
        "                img = Image.open(img_path)\n",
        "                img.verify()\n",
        "\n",
        "                # Get filename for label\n",
        "                filename = os.path.basename(img_path)\n",
        "\n",
        "                # Try to determine tumor type from filename\n",
        "                label = filename\n",
        "                if 'glioma' in filename.lower():\n",
        "                    label = f\"üìå Glioma - {filename}\"\n",
        "                elif 'meningioma' in filename.lower():\n",
        "                    label = f\"üìå Meningioma - {filename}\"\n",
        "                elif 'pituitary' in filename.lower():\n",
        "                    label = f\"üìå Pituitary - {filename}\"\n",
        "                elif 'notumor' in filename.lower() or 'no_tumor' in filename.lower() or 'normal' in filename.lower():\n",
        "                    label = f\"üìå Normal - {filename}\"\n",
        "                else:\n",
        "                    label = f\"üìå {filename}\"\n",
        "\n",
        "                sample_images.append([img_path, label])\n",
        "                print(f\"  ‚úì Loaded: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚úó Failed to load {os.path.basename(img_path)}: {str(e)}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(sample_images)} sample images\")\n",
        "    else:\n",
        "        print(f\"Sample directory not found: {sample_dir}\")\n",
        "\n",
        "    if not sample_images:\n",
        "        print(\"No sample images found - examples will not be available\")\n",
        "        return None\n",
        "\n",
        "    return [[img_path] for img_path, _ in sample_images], [label for _, label in sample_images]\n",
        "\n",
        "def initialize_models():\n",
        "    \"\"\"Initialize both models with better path detection\"\"\"\n",
        "    global classifier, segmenter\n",
        "\n",
        "    search_dirs = [\n",
        "        \"/content/drive/MyDrive/IP Project\",\n",
        "        \"/kaggle/working\",\n",
        "        \".\",\n",
        "        os.getcwd()\n",
        "    ]\n",
        "\n",
        "    # SAM checkpoint paths to try\n",
        "    sam_checkpoints = [\n",
        "        \"/content/drive/MyDrive/IP Project/sam_vit_b_01ec64.pth\",\n",
        "        \"/content/sam_vit_b_01ec64.pth\",\n",
        "        \"/kaggle/working/sam_vit_b_01ec64.pth\",\n",
        "        None  # Will proceed without SAM if not found\n",
        "    ]\n",
        "\n",
        "    # Find and load classification model\n",
        "    classification_found = False\n",
        "    for base_dir in search_dirs:\n",
        "        if not os.path.exists(base_dir):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(base_dir) if os.path.isdir(base_dir) else []:\n",
        "            if 'resnet' in file.lower() and file.endswith('.pth'):\n",
        "                try:\n",
        "                    model_path = os.path.join(base_dir, file)\n",
        "                    classifier = BrainTumorClassifier(model_path)\n",
        "                    print(f\"‚úì Classification model loaded from: {model_path}\")\n",
        "                    classification_found = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load {file}: {e}\")\n",
        "\n",
        "        if classification_found:\n",
        "            break\n",
        "\n",
        "    # Specific paths for classification model\n",
        "    if not classification_found:\n",
        "        specific_paths = [\n",
        "            \"/content/drive/MyDrive/IP Project/resnet101_brain_tumor_20251105_163019.pth\",\n",
        "            \"/kaggle/working/best_model.pth\"\n",
        "        ]\n",
        "        for path in specific_paths:\n",
        "            if os.path.exists(path):\n",
        "                try:\n",
        "                    classifier = BrainTumorClassifier(path)\n",
        "                    print(f\"‚úì Classification model loaded from: {path}\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load classification model: {e}\")\n",
        "\n",
        "    # Find and load segmentation model\n",
        "    segmentation_found = False\n",
        "    sam_checkpoint = None\n",
        "\n",
        "    # Find SAM checkpoint\n",
        "    for sam_path in sam_checkpoints:\n",
        "        if sam_path and os.path.exists(sam_path):\n",
        "            sam_checkpoint = sam_path\n",
        "            print(f\"‚úì SAM checkpoint found at: {sam_checkpoint}\")\n",
        "            break\n",
        "\n",
        "    if sam_checkpoint is None and SAM_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è SAM checkpoint not found - will proceed without SAM refinement\")\n",
        "\n",
        "    for base_dir in search_dirs:\n",
        "        if not os.path.exists(base_dir):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(base_dir) if os.path.isdir(base_dir) else []:\n",
        "            if ('segment' in file.lower() or 'unet' in file.lower()) and file.endswith('.pth'):\n",
        "                try:\n",
        "                    model_path = os.path.join(base_dir, file)\n",
        "                    segmenter = BrainTumorSegmenter(model_path, sam_checkpoint)\n",
        "                    print(f\"‚úì Segmentation model loaded from: {model_path}\")\n",
        "                    segmentation_found = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load {file}: {e}\")\n",
        "\n",
        "        if segmentation_found:\n",
        "            break\n",
        "\n",
        "    # Specific paths for segmentation model\n",
        "    if not segmentation_found:\n",
        "        specific_paths = [\n",
        "            \"/content/drive/MyDrive/IP Project/brain_tumor_segmentation_model.pth\",\n",
        "            \"/kaggle/working/brain_tumor_segmentation_model.pth\"\n",
        "        ]\n",
        "        for path in specific_paths:\n",
        "            if os.path.exists(path):\n",
        "                try:\n",
        "                    segmenter = BrainTumorSegmenter(path, sam_checkpoint)\n",
        "                    print(f\"‚úì Segmentation model loaded from: {path}\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load segmentation model: {e}\")\n",
        "\n",
        "    return classifier is not None, segmenter is not None\n",
        "\n",
        "# Initialize models on startup\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ Initializing Brain Tumor Analysis System...\")\n",
        "print(\"=\"*60)\n",
        "class_loaded, seg_loaded = initialize_models()\n",
        "print(\"=\"*60)\n",
        "print(f\"System Status:\")\n",
        "print(f\"  Classification Model: {'‚úÖ Ready' if class_loaded else '‚ùå Not Found'}\")\n",
        "print(f\"  Segmentation Model: {'‚úÖ Ready' if seg_loaded else '‚ùå Not Found'}\")\n",
        "print(f\"  SAM Refinement: {'‚úÖ Available' if (segmenter and segmenter.sam_predictor) else '‚ùå Not Available'}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load sample images\n",
        "print(\"\\nüìÅ Loading sample images...\")\n",
        "sample_data = load_sample_images()\n",
        "if sample_data:\n",
        "    sample_images, sample_labels = sample_data\n",
        "    print(f\"‚úÖ {len(sample_images)} sample images ready for testing\")\n",
        "else:\n",
        "    sample_images, sample_labels = None, None\n",
        "    print(\"‚ö†Ô∏è No sample images available\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def create_segmentation_figure(original_image, mask, prob_map, tumor_class, unet_mask=None):\n",
        "    \"\"\"Create enhanced segmentation visualization with SAM refinement comparison\"\"\"\n",
        "\n",
        "    if isinstance(original_image, Image.Image):\n",
        "        img_array = np.array(original_image.convert('RGB'))\n",
        "    else:\n",
        "        img_array = original_image.copy()\n",
        "\n",
        "    # Determine if SAM was used\n",
        "    sam_used = unet_mask is not None and mask is not None and not np.array_equal(mask, unet_mask)\n",
        "\n",
        "    if tumor_class.lower() != 'notumor' and mask is not None:\n",
        "        # Calculate tumor statistics\n",
        "        tumor_pixels = mask.sum()\n",
        "        total_pixels = mask.size\n",
        "        tumor_percentage = (tumor_pixels / total_pixels) * 100\n",
        "\n",
        "        if sam_used:\n",
        "            # Create figure with 3 subplots for comparison\n",
        "            fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "            # 1. U-Net ONLY (Left)\n",
        "            plt.subplot(1, 3, 1)\n",
        "            overlay_unet = img_array.copy()\n",
        "            red_overlay = np.zeros_like(img_array)\n",
        "            red_overlay[:, :, 0] = 255\n",
        "\n",
        "            for c in range(3):\n",
        "                overlay_unet[:, :, c] = np.where(unet_mask == 1,\n",
        "                                           img_array[:, :, c] * 0.6 + red_overlay[:, :, c] * 0.4,\n",
        "                                           img_array[:, :, c])\n",
        "\n",
        "            alpha = 0.4\n",
        "            blended_unet = cv2.addWeighted(img_array, 1-alpha, overlay_unet.astype(np.uint8), alpha, 0)\n",
        "            plt.imshow(blended_unet)\n",
        "            plt.title('U-Net Segmentation', fontsize=14, fontweight='bold')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 2. SAM REFINED (Middle)\n",
        "            plt.subplot(1, 3, 2)\n",
        "            overlay_sam = img_array.copy()\n",
        "            green_overlay = np.zeros_like(img_array)\n",
        "            green_overlay[:, :, 1] = 255\n",
        "\n",
        "            for c in range(3):\n",
        "                overlay_sam[:, :, c] = np.where(mask == 1,\n",
        "                                           img_array[:, :, c] * 0.6 + green_overlay[:, :, c] * 0.4,\n",
        "                                           img_array[:, :, c])\n",
        "\n",
        "            blended_sam = cv2.addWeighted(img_array, 1-alpha, overlay_sam.astype(np.uint8), alpha, 0)\n",
        "            plt.imshow(blended_sam)\n",
        "            plt.title('SAM Refined Segmentation', fontsize=14, fontweight='bold', color='green')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 3. COMPARISON (Right)\n",
        "            plt.subplot(1, 3, 3)\n",
        "            comparison_img = img_array.copy()\n",
        "\n",
        "            # Show U-Net contours in red\n",
        "            contours_unet, _ = cv2.findContours(unet_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(comparison_img, contours_unet, -1, (255, 0, 0), 2)\n",
        "\n",
        "            # Show SAM contours in green\n",
        "            contours_sam, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(comparison_img, contours_sam, -1, (0, 255, 0), 2)\n",
        "\n",
        "            plt.imshow(comparison_img)\n",
        "            plt.title('Comparison (Red: U-Net, Green: SAM)', fontsize=14, fontweight='bold')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Calculate improvement\n",
        "            unet_pixels = unet_mask.sum()\n",
        "            improvement = ((tumor_pixels - unet_pixels) / unet_pixels * 100) if unet_pixels > 0 else 0\n",
        "\n",
        "            plt.suptitle(\n",
        "                f'{tumor_class.upper()} TUMOR - SAM REFINED\\n' +\n",
        "                f'Coverage: {tumor_percentage:.1f}% | Refinement: {improvement:+.1f}%',\n",
        "                fontsize=16,\n",
        "                fontweight='bold',\n",
        "                color='darkgreen'\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Original 2-subplot layout when SAM not used\n",
        "            fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "            # 1. BLENDED VIEW (Left)\n",
        "            plt.subplot(1, 2, 1)\n",
        "            overlay = img_array.copy()\n",
        "            red_overlay = np.zeros_like(img_array)\n",
        "            red_overlay[:, :, 0] = 255\n",
        "\n",
        "            for c in range(3):\n",
        "                overlay[:, :, c] = np.where(mask == 1,\n",
        "                                           img_array[:, :, c] * 0.6 + red_overlay[:, :, c] * 0.4,\n",
        "                                           img_array[:, :, c])\n",
        "\n",
        "            alpha = 0.4\n",
        "            blended = cv2.addWeighted(img_array, 1-alpha, overlay.astype(np.uint8), alpha, 0)\n",
        "            plt.imshow(blended)\n",
        "            plt.title('Tumor Region Overlay', fontsize=14, fontweight='bold')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 2. TUMOR BOUNDARIES (Right)\n",
        "            plt.subplot(1, 2, 2)\n",
        "            contour_img = img_array.copy()\n",
        "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 3)\n",
        "\n",
        "            overlay_contour = img_array.copy()\n",
        "            cv2.fillPoly(overlay_contour, contours, (0, 255, 0))\n",
        "            contour_img = cv2.addWeighted(contour_img, 0.8, overlay_contour, 0.2, 0)\n",
        "\n",
        "            plt.imshow(contour_img)\n",
        "            plt.title('Tumor Boundaries', fontsize=14, fontweight='bold')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.suptitle(\n",
        "                f'{tumor_class.upper()} TUMOR DETECTED\\nEstimated Coverage: {tumor_percentage:.1f}% of Brain Area',\n",
        "                fontsize=16,\n",
        "                fontweight='bold',\n",
        "                color='darkred'\n",
        "            )\n",
        "    else:\n",
        "        # No tumor case\n",
        "        fig = plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.imshow(img_array)\n",
        "        plt.title('NO TUMOR DETECTED', fontsize=18, fontweight='bold', color='green')\n",
        "        plt.text(0.5, -0.05, 'Brain tissue appears healthy',\n",
        "                ha='center', transform=plt.gca().transAxes,\n",
        "                fontsize=12, style='italic', color='green')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=120, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
        "    plt.close('all')\n",
        "    buf.seek(0)\n",
        "\n",
        "    return Image.open(buf)\n",
        "\n",
        "def create_probability_plot(probabilities):\n",
        "    \"\"\"Create probability bar chart\"\"\"\n",
        "    if probabilities is None:\n",
        "        return None\n",
        "\n",
        "    classes = list(probabilities.keys())\n",
        "    probs = list(probabilities.values())\n",
        "\n",
        "    # Color coding\n",
        "    colors = []\n",
        "    for i, p in enumerate(probs):\n",
        "        if p == max(probs):\n",
        "            if classes[i].lower() == 'notumor':\n",
        "                colors.append('#2ecc71')  # Green for no tumor\n",
        "            else:\n",
        "                colors.append('#e74c3c')  # Red for tumor\n",
        "        else:\n",
        "            colors.append('#95a5a6')  # Gray for other classes\n",
        "\n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(\n",
        "            x=probs,\n",
        "            y=[c.upper() for c in classes],\n",
        "            orientation='h',\n",
        "            marker=dict(color=colors),\n",
        "            text=[f'{p:.1f}%' for p in probs],\n",
        "            textposition='outside',\n",
        "            hovertemplate='<b>%{y}</b><br>Probability: %{x:.2f}%<extra></extra>'\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': \"Classification Confidence Scores\",\n",
        "            'font': {'size': 16, 'color': '#2c3e50'}\n",
        "        },\n",
        "        xaxis_title=\"Probability (%)\",\n",
        "        yaxis_title=\"Tumor Type\",\n",
        "        xaxis=dict(range=[0, 105]),\n",
        "        height=350,\n",
        "        template=\"plotly_white\",\n",
        "        showlegend=False,\n",
        "        margin=dict(l=100, r=20, t=50, b=50)\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def analyze_brain_mri(image, use_sam):\n",
        "    \"\"\"Main analysis function with SAM refinement option\"\"\"\n",
        "\n",
        "    if classifier is None:\n",
        "        return (\n",
        "            \"‚ùå Model Not Loaded\",\n",
        "            \"\",\n",
        "            None,\n",
        "            None,\n",
        "            \"### ‚ö†Ô∏è Classification Model Not Found\\nPlease ensure the model file exists.\"\n",
        "        )\n",
        "\n",
        "    if image is None:\n",
        "        return (\n",
        "            \"Awaiting Upload\",\n",
        "            \"\",\n",
        "            None,\n",
        "            None,\n",
        "            \"### üì§ Upload Required\\nPlease upload a brain MRI scan or select a sample image.\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Perform classification\n",
        "        predicted_class, confidence, probabilities = classifier.predict(image)\n",
        "\n",
        "        # Format diagnosis\n",
        "        if predicted_class.lower() == 'notumor':\n",
        "            diagnosis = f\"‚úÖ HEALTHY\"\n",
        "            emoji = \"‚úÖ\"\n",
        "            status_color = \"green\"\n",
        "        else:\n",
        "            diagnosis = f\"‚ö†Ô∏è {predicted_class.upper()} TUMOR\"\n",
        "            emoji = \"‚ö†Ô∏è\"\n",
        "            status_color = \"red\"\n",
        "\n",
        "        confidence_text = f\"{confidence:.1f}%\"\n",
        "\n",
        "        # Create probability chart\n",
        "        prob_chart = create_probability_plot(probabilities)\n",
        "\n",
        "        # Perform segmentation\n",
        "        seg_image = None\n",
        "        seg_status = \"Not Performed\"\n",
        "        refinement_status = \"\"\n",
        "\n",
        "        if segmenter and segmenter.model is not None:\n",
        "            if predicted_class.lower() != 'notumor':\n",
        "                # Use SAM refinement based on checkbox\n",
        "                result = segmenter.segment(image, use_sam_refinement=use_sam)\n",
        "\n",
        "                if len(result) == 3:\n",
        "                    mask, prob_map, unet_mask = result\n",
        "                else:\n",
        "                    mask, prob_map = result\n",
        "                    unet_mask = None\n",
        "\n",
        "                if mask is not None:\n",
        "                    seg_image = create_segmentation_figure(image, mask, prob_map, predicted_class, unet_mask)\n",
        "\n",
        "                    if use_sam and segmenter.sam_predictor and unet_mask is not None:\n",
        "                        seg_status = \"‚úÖ Tumor Localized with SAM Refinement\"\n",
        "                        refinement_status = \" (SAM-Enhanced)\"\n",
        "                    else:\n",
        "                        seg_status = \"‚úÖ Tumor Localized with U-Net\"\n",
        "                        refinement_status = \" (U-Net Only)\"\n",
        "                else:\n",
        "                    seg_image = image\n",
        "                    seg_status = \"‚ö†Ô∏è Segmentation Failed\"\n",
        "            else:\n",
        "                seg_image = create_segmentation_figure(image, None, None, predicted_class)\n",
        "                seg_status = \"‚úÖ No Segmentation Needed (Healthy)\"\n",
        "        else:\n",
        "            seg_image = image if isinstance(image, Image.Image) else Image.fromarray(image)\n",
        "            seg_status = \"‚ùå Segmentation Model Unavailable\"\n",
        "\n",
        "        # Generate interpretation\n",
        "        interpretation = f\"\"\"\n",
        "## {emoji} Analysis Results\n",
        "\n",
        "### üîç Primary Diagnosis\n",
        "- **Classification:** {predicted_class.capitalize()}\n",
        "- **Confidence:** {confidence:.1f}%\n",
        "- **Segmentation:** {seg_status}{refinement_status}\n",
        "\n",
        "### üìä Class Probabilities\n",
        "\"\"\"\n",
        "\n",
        "        # Add probability breakdown\n",
        "        for cls, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
        "            if cls == predicted_class:\n",
        "                interpretation += f\"\\n**‚Üí {cls.capitalize()}: {prob:.1f}%** *(Detected)*\"\n",
        "            else:\n",
        "                interpretation += f\"\\n   {cls.capitalize()}: {prob:.1f}%\"\n",
        "\n",
        "        # Add SAM refinement info if used\n",
        "        if use_sam and segmenter and segmenter.sam_predictor:\n",
        "            interpretation += \"\\n\\n### üéØ SAM Refinement Applied\\n\"\n",
        "            interpretation += \"- Using Segment Anything Model for enhanced boundary detection\\n\"\n",
        "            interpretation += \"- Bounding box extracted from U-Net prediction\\n\"\n",
        "            interpretation += \"- SAM refined segmentation within tumor region\"\n",
        "\n",
        "        # Add clinical notes\n",
        "        clinical_info = {\n",
        "            'glioma': \"\"\"\n",
        "### üè• Clinical Notes: GLIOMA\n",
        "- **Type:** Primary brain tumor from glial cells\n",
        "- **Action:** Urgent neurology referral recommended\n",
        "- **Treatment:** May include surgery, radiation, chemotherapy\"\"\",\n",
        "            'meningioma': \"\"\"\n",
        "### üè• Clinical Notes: MENINGIOMA\n",
        "- **Type:** Usually benign, slow-growing tumor\n",
        "- **Action:** Neurosurgical consultation advised\n",
        "- **Treatment:** Monitoring or surgical removal\"\"\",\n",
        "            'pituitary': \"\"\"\n",
        "### üè• Clinical Notes: PITUITARY TUMOR\n",
        "- **Type:** Adenoma affecting hormone production\n",
        "- **Action:** Endocrinology evaluation needed\n",
        "- **Treatment:** Medication, surgery, or radiation\"\"\",\n",
        "            'notumor': \"\"\"\n",
        "### ‚úÖ Clinical Notes: HEALTHY BRAIN\n",
        "- **Finding:** No abnormalities detected\n",
        "- **Action:** Continue routine health monitoring\n",
        "- **Note:** Regular checkups still recommended\"\"\"\n",
        "        }\n",
        "\n",
        "        if predicted_class.lower() in clinical_info:\n",
        "            interpretation += f\"\\n{clinical_info[predicted_class.lower()]}\"\n",
        "\n",
        "        interpretation += f\"\"\"\n",
        "\n",
        "---\n",
        "**‚ö†Ô∏è Medical Disclaimer:** This AI analysis is for screening purposes only. Always consult qualified healthcare professionals for diagnosis and treatment.\n",
        "\n",
        "*Analysis performed: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}*\n",
        "*Model: {classifier.model_name.upper()} | Accuracy: {classifier.test_accuracy:.1f}%*\n",
        "\"\"\"\n",
        "\n",
        "        return diagnosis, confidence_text, prob_chart, seg_image, interpretation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during analysis: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return (\n",
        "            \"‚ùå Error\",\n",
        "            \"\",\n",
        "            None,\n",
        "            None,\n",
        "            f\"### Analysis Error\\n```\\n{str(e)}\\n```\"\n",
        "        )\n",
        "\n",
        "# Create Gradio Interface\n",
        "with gr.Blocks(\n",
        "    title=\"üß† Brain Tumor AI Analysis\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container {font-family: 'Inter', 'Arial', sans-serif;}\n",
        "    .output-class {font-size: 22px !important; font-weight: bold !important; color: #2c3e50;}\n",
        "    .sample-gallery {margin-top: 15px; border-radius: 8px;}\n",
        "    h1 {color: #2c3e50 !important;}\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üß† AI Brain Tumor Detection & Localization System\n",
        "\n",
        "    **Advanced Deep Learning Analysis** - Instant classification and visual tumor localization with SAM refinement for brain MRI scans.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            input_image = gr.Image(\n",
        "                label=\"üì§ Upload Brain MRI\",\n",
        "                type=\"pil\",\n",
        "                elem_id=\"input-image\"\n",
        "            )\n",
        "\n",
        "            # SAM refinement checkbox\n",
        "            use_sam_refinement = gr.Checkbox(\n",
        "                label=\"üéØ Enable SAM Refinement\",\n",
        "                value=True if (segmenter and segmenter.sam_predictor) else False,\n",
        "                interactive=True if (segmenter and segmenter.sam_predictor) else False,\n",
        "                info=\"Use Segment Anything Model to refine tumor boundaries\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                analyze_btn = gr.Button(\n",
        "                    \"üîç Analyze MRI\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\",\n",
        "                    elem_id=\"analyze-button\"\n",
        "                )\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear\", variant=\"secondary\")\n",
        "\n",
        "            # Sample images section\n",
        "            if sample_images and sample_labels:\n",
        "                gr.Markdown(\"### üñºÔ∏è Quick Test Samples\")\n",
        "                gr.Markdown(\"*Click any sample to load it*\")\n",
        "\n",
        "                examples = gr.Examples(\n",
        "                    examples=sample_images,\n",
        "                    inputs=input_image,\n",
        "                    label=\"Available Samples\",\n",
        "                    examples_per_page=5,\n",
        "                )\n",
        "\n",
        "            # System status\n",
        "            gr.Markdown(f\"\"\"\n",
        "            ### üíª System Status\n",
        "            Classification: {'üü¢ Online' if classifier else 'üî¥ Offline'}\n",
        "            Segmentation: {'üü¢ Online' if (segmenter and segmenter.model) else 'üî¥ Offline'}\n",
        "            SAM Refinement: {'üü¢ Ready' if (segmenter and segmenter.sam_predictor) else 'üî¥ Not Available'}\n",
        "            Samples: {'‚úÖ Loaded' if sample_images else '‚ùå None'}\n",
        "            Device: {'üöÄ GPU' if torch.cuda.is_available() else 'üíª CPU'}\n",
        "            \"\"\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Row():\n",
        "                diagnosis_output = gr.Textbox(\n",
        "                    label=\"üìã Diagnosis\",\n",
        "                    elem_classes=\"output-class\"\n",
        "                )\n",
        "                confidence_output = gr.Textbox(\n",
        "                    label=\"üéØ Confidence\",\n",
        "                )\n",
        "\n",
        "            prob_chart = gr.Plot(label=\"üìä Classification Analysis\")\n",
        "\n",
        "            seg_output = gr.Image(\n",
        "                label=\"üîç Tumor Visualization (Enhanced with SAM)\",\n",
        "                type=\"pil\"\n",
        "            )\n",
        "\n",
        "            interpretation = gr.Markdown(label=\"üìù Detailed Report\")\n",
        "\n",
        "    # Event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=analyze_brain_mri,\n",
        "        inputs=[input_image, use_sam_refinement],\n",
        "        outputs=[diagnosis_output, confidence_output, prob_chart, seg_output, interpretation]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (None, True if (segmenter and segmenter.sam_predictor) else False, \"\", \"\", None, None, \"\"),\n",
        "        inputs=[],\n",
        "        outputs=[input_image, use_sam_refinement, diagnosis_output, confidence_output, prob_chart, seg_output, interpretation]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### üìö System Information\n",
        "\n",
        "    **Capabilities:**\n",
        "    - üéØ 4-Class tumor classification (Glioma, Meningioma, Pituitary, No Tumor)\n",
        "    - üîç Automatic tumor region visualization with U-Net\n",
        "    - üéØ SAM refinement for enhanced boundary detection\n",
        "    - üìä Confidence scoring across all classes\n",
        "\n",
        "    **Visualization Modes:**\n",
        "    - **U-Net Only:** Fast segmentation using trained U-Net model\n",
        "    - **SAM Refined:** Enhanced boundaries using Segment Anything Model\n",
        "    - **Comparison View:** Side-by-side comparison when SAM is enabled\n",
        "\n",
        "    *For medical professionals and research purposes only.*\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üöÄ Launching Brain Tumor Analysis System...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if sample_images:\n",
        "        print(f\"‚úÖ {len(sample_images)} sample images ready\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No sample images available\")\n",
        "\n",
        "    if segmenter and segmenter.sam_predictor:\n",
        "        print(\"‚úÖ SAM refinement capability enabled\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è SAM refinement not available\")\n",
        "\n",
        "    print(\"\\nüì± Starting web interface...\")\n",
        "    demo.launch(share=True, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xk7Os-aouwvJ",
        "outputId": "dcb3ee64-f5ea-445d-ccc2-77a9e2ec73b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ Initializing Brain Tumor Analysis System...\n",
            "============================================================\n",
            "Loading classification model from: /content/drive/MyDrive/IP Project/resnet101_brain_tumor_20251105_163019.pth\n",
            "‚úì Classification model loaded successfully!\n",
            "  Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "‚úì Classification model loaded from: /content/drive/MyDrive/IP Project/resnet101_brain_tumor_20251105_163019.pth\n",
            "‚úì SAM checkpoint found at: /content/sam_vit_b_01ec64.pth\n",
            "Loading segmentation model from: /content/drive/MyDrive/IP Project/brain_tumor_segmentation_model.pth\n",
            "  Creating U-Net with efficientnet-b3 encoder...\n",
            "‚úì Segmentation model loaded successfully!\n",
            "Loading SAM model from: /content/sam_vit_b_01ec64.pth\n",
            "‚úì SAM model (vit_b) loaded successfully!\n",
            "‚úì Segmentation model loaded from: /content/drive/MyDrive/IP Project/brain_tumor_segmentation_model.pth\n",
            "============================================================\n",
            "System Status:\n",
            "  Classification Model: ‚úÖ Ready\n",
            "  Segmentation Model: ‚úÖ Ready\n",
            "  SAM Refinement: ‚úÖ Available\n",
            "============================================================\n",
            "\n",
            "üìÅ Loading sample images...\n",
            "Loading sample images from: /content/drive/MyDrive/IP Project/sample_images\n",
            "  ‚úì Loaded: Tr-gl_0011.jpg\n",
            "  ‚úì Loaded: Tr-me_0010.jpg\n",
            "  ‚úì Loaded: Tr-no_0010.jpg\n",
            "  ‚úì Loaded: Tr-pi_0010.jpg\n",
            "Successfully loaded 4 sample images\n",
            "‚úÖ 4 sample images ready for testing\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "üöÄ Launching Brain Tumor Analysis System...\n",
            "============================================================\n",
            "‚úÖ 4 sample images ready\n",
            "‚úÖ SAM refinement capability enabled\n",
            "\n",
            "üì± Starting web interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://12d7a4d86d84f5cbbd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://12d7a4d86d84f5cbbd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}